{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "logisticRegression.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyO42QoaehY3EYGcg/HSUXGK",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SH0123/DeepLearning/blob/master/logisticRegression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fTSiEqdYCdnI",
        "colab_type": "code",
        "outputId": "49be6be4-7dae-43f8-be91-8bb4fa304acd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "torch.manual_seed(1)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7f4a7614e310>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FpMu76mNCqba",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class BinaryClassifier(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.linear = nn.Linear(2, 1)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.sigmoid(self.linear(x))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ytCaIeRATqn4",
        "colab_type": "code",
        "outputId": "50be48f9-74cb-4082-89ac-a42cc03969d7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 212
        }
      },
      "source": [
        "x_data = torch.FloatTensor([[1, 2], [2, 3], [3, 1], [4, 3], [5, 3], [6, 2]])\n",
        "y_data = torch.FloatTensor([[0], [0], [0], [1], [1], [1]])\n",
        "\n",
        "model = BinaryClassifier()\n",
        "\n",
        "optimizer = optim.SGD(model.parameters(), lr=1)\n",
        "\n",
        "nb_epochs = 100\n",
        "for epoch in range(nb_epochs + 1):\n",
        "\n",
        "    hypothesis = model(x_data)\n",
        "\n",
        "    cost = F.binary_cross_entropy(hypothesis, y_data)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    cost.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if epoch % 10 == 0:\n",
        "        prediction = hypothesis >= torch.FloatTensor([0.5])\n",
        "        correct_prediction = prediction.float() == y_data\n",
        "        accuracy = correct_prediction.sum().item() / len(correct_prediction)\n",
        "        print('Epoch {:4d}/{} Cost: {:.6f} Accuracy {:2.2f}%'.format(\n",
        "            epoch, nb_epochs, cost.item(), accuracy * 100\n",
        "        ))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch    0/100 Cost: 0.778947 Accuracy 33.33%\n",
            "Epoch   10/100 Cost: 0.606802 Accuracy 66.67%\n",
            "Epoch   20/100 Cost: 0.446548 Accuracy 66.67%\n",
            "Epoch   30/100 Cost: 0.376169 Accuracy 83.33%\n",
            "Epoch   40/100 Cost: 0.318945 Accuracy 83.33%\n",
            "Epoch   50/100 Cost: 0.268428 Accuracy 83.33%\n",
            "Epoch   60/100 Cost: 0.222594 Accuracy 100.00%\n",
            "Epoch   70/100 Cost: 0.183695 Accuracy 100.00%\n",
            "Epoch   80/100 Cost: 0.158160 Accuracy 100.00%\n",
            "Epoch   90/100 Cost: 0.144616 Accuracy 100.00%\n",
            "Epoch  100/100 Cost: 0.134716 Accuracy 100.00%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SsGhuUBoVLbP",
        "colab_type": "code",
        "outputId": "45c7a965-15e2-4186-983f-3907c0816e18",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 159
        }
      },
      "source": [
        "#Cross Entropy Loss(low level)\n",
        "\n",
        "z = torch.rand(3, 5, requires_grad=True)\n",
        "hypothesis = F.softmax(z, dim=1)\n",
        "\n",
        "#[3,]\n",
        "y = torch.randint(5, (3,)).long()\n",
        "\n",
        "y_one_hot = torch.zeros_like(hypothesis)\n",
        "#[3, 5]\n",
        "y_one_hot.scatter_(1, y.unsqueeze(1), 1)\n",
        "\n",
        "#[3, 1]\n",
        "cost = (y_one_hot * -torch.log(hypothesis)).sum(dim=1)\n",
        "\"\"\"\n",
        "F.nll_loss(F.log_softmax(z, dim=1), y)로 나타낼 수 있음\n",
        "F.cross_entropy(z, y)가 가장 간단\n",
        "\"\"\"\n",
        "#scalar\n",
        "cost = cost.mean()\n",
        "\n",
        "print(hypothesis)\n",
        "print(y)\n",
        "print(y_one_hot)\n",
        "print(cost)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[0.2528, 0.1500, 0.2019, 0.1292, 0.2662],\n",
            "        [0.1518, 0.2729, 0.1887, 0.1441, 0.2425],\n",
            "        [0.1761, 0.2659, 0.1987, 0.2173, 0.1420]], grad_fn=<SoftmaxBackward>)\n",
            "tensor([0, 3, 0])\n",
            "tensor([[1., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 1., 0.],\n",
            "        [1., 0., 0., 0., 0.]])\n",
            "tensor(1.6830, grad_fn=<MeanBackward0>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WrczUtlLVm04",
        "colab_type": "code",
        "outputId": "06815b4f-232f-4f0d-e59b-b4fbdf15f6c0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 212
        }
      },
      "source": [
        "#Training Cross Entropy Loss(high level)\n",
        "\n",
        "class SoftmaxClassifierModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.linear = nn.Linear(4, 3)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        return self.linear(x)\n",
        "\n",
        "x_train = torch.FloatTensor([[1, 2, 1, 1],\n",
        "                            [2, 1, 3, 2],\n",
        "                            [3, 1, 3, 4],\n",
        "                            [4, 1, 5, 5],\n",
        "                            [1, 7, 5, 5],\n",
        "                            [1, 2, 5, 6],\n",
        "                            [1, 6, 6, 6],\n",
        "                            [1, 7, 7, 7]])\n",
        "y_train = torch.LongTensor([2, 2, 2, 1, 1, 1, 0, 0])\n",
        "\n",
        "model = SoftmaxClassifierModel()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
        "\n",
        "nb_epochs = 1000\n",
        "for epoch in range(nb_epochs + 1):\n",
        "\n",
        "    pred = model(x_train)\n",
        "    cost = F.cross_entropy(pred, y_train)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    cost.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if epoch % 100 == 0:\n",
        "        print('Epoch {:4d}/{} Cost: {:.6f}'.format(\n",
        "            epoch, nb_epochs, cost.item()\n",
        "        ))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch    0/1000 Cost: 1.193624\n",
            "Epoch  100/1000 Cost: 0.647261\n",
            "Epoch  200/1000 Cost: 0.559104\n",
            "Epoch  300/1000 Cost: 0.503264\n",
            "Epoch  400/1000 Cost: 0.458217\n",
            "Epoch  500/1000 Cost: 0.418269\n",
            "Epoch  600/1000 Cost: 0.380891\n",
            "Epoch  700/1000 Cost: 0.344497\n",
            "Epoch  800/1000 Cost: 0.307935\n",
            "Epoch  900/1000 Cost: 0.271172\n",
            "Epoch 1000/1000 Cost: 0.243551\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v5hAFDVOUGT-",
        "colab_type": "text"
      },
      "source": [
        "multi layer perceptron을 통해 XOR 문제 해결"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k7YaUyQZae9h",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "b26cbb86-133d-4b15-d495-0bdeb43624ed"
      },
      "source": [
        "x = torch.FloatTensor([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
        "y = torch.FloatTensor([0, 1, 1, 0])\n",
        "\n",
        "class MultiPercep(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.linear1 = nn.Linear(2, 2, bias=True)\n",
        "        self.linear2 = nn.Linear(2, 1, bias=True)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "        self.percep = nn.Sequential(self.linear1, self.sigmoid, self.linear2, self.sigmoid)\n",
        "\n",
        "    def forward(self, input):\n",
        "        output = self.percep(input)\n",
        "\n",
        "        return output\n",
        "\n",
        "criterion = nn.BCELoss()\n",
        "model = MultiPercep()\n",
        "optimizer = optim.SGD(model.parameters(), lr=1)\n",
        "\n",
        "nb_epochs = 10000\n",
        "for epoch in range(nb_epochs + 1):\n",
        "    \n",
        "    hypothesis = model(x)\n",
        "    cost = criterion(hypothesis, y)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    cost.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if epoch % 100 == 0:\n",
        "        print(epoch, cost.item())\n",
        "\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:498: UserWarning: Using a target size (torch.Size([4])) that is different to the input size (torch.Size([4, 1])) is deprecated. Please ensure they have the same size.\n",
            "  return F.binary_cross_entropy(input, target, weight=self.weight, reduction=self.reduction)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "0 0.693722128868103\n",
            "100 0.6924341917037964\n",
            "200 0.6889626383781433\n",
            "300 0.6674384474754333\n",
            "400 0.5770507454872131\n",
            "500 0.4295453429222107\n",
            "600 0.15795214474201202\n",
            "700 0.07011382281780243\n",
            "800 0.0428619459271431\n",
            "900 0.03044535219669342\n",
            "1000 0.023469213396310806\n",
            "1100 0.019035127013921738\n",
            "1200 0.01598052866756916\n",
            "1300 0.013754032552242279\n",
            "1400 0.012061947956681252\n",
            "1500 0.010734030045568943\n",
            "1600 0.009665104560554028\n",
            "1700 0.008786661550402641\n",
            "1800 0.008052390068769455\n",
            "1900 0.007429709192365408\n",
            "2000 0.006895182654261589\n",
            "2100 0.0064313896000385284\n",
            "2200 0.006025277078151703\n",
            "2300 0.005666794255375862\n",
            "2400 0.005348098464310169\n",
            "2500 0.005062920041382313\n",
            "2600 0.0048062740825116634\n",
            "2700 0.00457411166280508\n",
            "2800 0.004363107495009899\n",
            "2900 0.004170479718595743\n",
            "3000 0.003993973135948181\n",
            "3100 0.0038316799327731133\n",
            "3200 0.0036819472443312407\n",
            "3300 0.0035433489829301834\n",
            "3400 0.0034146986436098814\n",
            "3500 0.003294976195320487\n",
            "3600 0.0031832812819629908\n",
            "3700 0.0030788485892117023\n",
            "3800 0.0029809887055307627\n",
            "3900 0.0028891314286738634\n",
            "4000 0.002802707254886627\n",
            "4100 0.002721236553043127\n",
            "4200 0.0026443449314683676\n",
            "4300 0.0025716274976730347\n",
            "4400 0.0025027997326105833\n",
            "4500 0.002437502145767212\n",
            "4600 0.0023754951544106007\n",
            "4700 0.0023165689781308174\n",
            "4800 0.0022604691330343485\n",
            "4900 0.0022069860715419054\n",
            "5000 0.002155969850718975\n",
            "5100 0.0021072414238005877\n",
            "5200 0.0020606655161827803\n",
            "5300 0.0020160628482699394\n",
            "5400 0.001973313745111227\n",
            "5500 0.0019323432352393866\n",
            "5600 0.0018930465448647738\n",
            "5700 0.0018552741967141628\n",
            "5800 0.0018190259579569101\n",
            "5900 0.0017841377994045615\n",
            "6000 0.001750519615598023\n",
            "6100 0.0017181714065372944\n",
            "6200 0.0016869737301021814\n",
            "6300 0.0016568814171478152\n",
            "6400 0.0016278201946988702\n",
            "6500 0.0015997744631022215\n",
            "6600 0.0015726997517049313\n",
            "6700 0.001546476618386805\n",
            "6800 0.0015211047139018774\n",
            "6900 0.0014965992886573076\n",
            "7000 0.001472825650125742\n",
            "7100 0.001449813600629568\n",
            "7200 0.0014274438144639134\n",
            "7300 0.001405805698595941\n",
            "7400 0.0013847948284819722\n",
            "7500 0.0013644261052832007\n",
            "7600 0.00134460988920182\n",
            "7700 0.0013253760989755392\n",
            "7800 0.0013066497631371021\n",
            "7900 0.0012884909519925714\n",
            "8000 0.0012707801070064306\n",
            "8100 0.0012535767164081335\n",
            "8200 0.0012368063908070326\n",
            "8300 0.0012205139501020312\n",
            "8400 0.0012046094052493572\n",
            "8500 0.0011891380418092012\n",
            "8600 0.0011740397894755006\n",
            "8700 0.001159329665824771\n",
            "8800 0.001144947949796915\n",
            "8900 0.0011309543624520302\n",
            "9000 0.0011172592639923096\n",
            "9100 0.00110395229421556\n",
            "9200 0.0010909141274169087\n",
            "9300 0.0010781593155115843\n",
            "9400 0.001065748045220971\n",
            "9500 0.001053590327501297\n",
            "9600 0.0010416863951832056\n",
            "9700 0.0010300660505890846\n",
            "9800 0.001018744194880128\n",
            "9900 0.0010076162870973349\n",
            "10000 0.0009967571822926402\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "THoQ-mQ6XQeD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}