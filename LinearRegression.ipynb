{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LinearRegression.ipynb",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPOu9g3znedAPdz6Hvx76vK",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SH0123/DeepLearning/blob/master/LinearRegression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t0t-1rQdFAjq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import torch.optim as optim"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MXMJaP-KFGz6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_train = torch.FloatTensor([[1], [2], [3]])\n",
        "y_train = torch.FloatTensor([[2], [4], [6]])\n",
        "\n",
        "#requires_grad=True 를 통해 해당 variable을 학습시킬 것 임을 명시\n",
        "W = torch.zeros(1, requires_grad=True)\n",
        "b = torch.zeros(1, requires_grad=True)\n",
        "\n",
        "optimizer = optim.SGD([W, b], lr=0.01)\n",
        "\n",
        "nb_epochs = 1000\n",
        "for epoch in range(1, nb_epochs + 1):\n",
        "    hypothesis = x_train*W + b\n",
        "    cost = torch.mean((hypothesis - y_train) ** 2)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    cost.backward()\n",
        "    optimizer.step()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rTiuMP-sSJXB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#multi_Variate_linear_regression\n",
        "x_train = torch.FloatTensor([[73, 80, 75],\n",
        "                             [93, 88, 93],\n",
        "                             [89, 91, 90],\n",
        "                             [96, 98, 100],\n",
        "                             [73, 66, 70]])\n",
        "y_train = torch.FloatTensor([[152],[185],[180],[196],[142]])\n",
        "\n",
        "#initializing model\n",
        "W = torch.zeros((3, 1), requires_grad=True)\n",
        "b = torch.zeros(1, requires_grad=True)\n",
        "\"\"\"\n",
        "앞으로는 initializing model 아래와 같이 할 것임\n",
        "\n",
        "class MultivariateLinear(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.linear = nn.Linear(3, 1)\n",
        "\n",
        "    def forward(sef, x):\n",
        "        return self.linear(x)\n",
        "\n",
        "위와같이 선언 후\n",
        "model = MultivariateLinear()로 initialize\n",
        "hypothesis = model(x_train)\n",
        "\"\"\"\n",
        "\n",
        "optimizer = optim.SGD([W, b], lr=1e-5)\n",
        "\n",
        "nb_epochs = 20\n",
        "for epoch in range(nb_epochs + 1):\n",
        "\n",
        "    hypothesis = x_train.matmul(W) + b\n",
        "\n",
        "    cost = torch.mean((hypothesis - y_train) ** 2)\n",
        "    #cost = F.mes_loss(hypothesis, y_train)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    cost.backward()\n",
        "    optimizer.step()\n",
        "    #hypothesis.shape() >>> [5, 1]\n",
        "    print('Epoch {:4d}/{} hypothesis: {}, Cost: {:.6f}'.format(\n",
        "        epoch, nb_epochs, hypothesis.squeeze().detach(), cost.item()\n",
        "    ))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "56P2xVhaIQa6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#gradient descent 구현 \n",
        "#데이터\n",
        "x_train = torch.FloatTensor([[1], [2], [3]])\n",
        "y_train = torch.FloatTensor([[1], [2], [3]])\n",
        "#initializing model\n",
        "W = torch.zeros(1)\n",
        "#initializing learning rate\n",
        "lr = 0.1\n",
        "\n",
        "nb_epochs = 10\n",
        "for epoch in range(nb_epochs + 1):\n",
        "\n",
        "    #H(x) 계산\n",
        "    hypothesis = x_train * W\n",
        "\n",
        "    #cost gradient 계산\n",
        "    cost = torch.mean((hypothesis - y_train) ** 2)\n",
        "    gradient = torch.sum((W * x_train - y_train) * x_train)\n",
        "\n",
        "    #tensor를 scalar로 바꿀 때 .item()사용\n",
        "    print('Epoch {:4d}/{} W: {:.3f}, Cost: {:.6f}'.format(\n",
        "        epoch, nb_epochs, W.item(), cost.item()\n",
        "    ))\n",
        "\n",
        "    W -= lr * gradient"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sWdjx8LNDB-2",
        "colab_type": "text"
      },
      "source": [
        "아래는 data loading 공부를 위해 간략히 표현하고 정리해둔 code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D812paLkek3y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# how to dataLoad\n",
        "# basic dataLoading\n",
        "# 공부를 위한 data loading code\n",
        "\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "class practiceDataset(Dataset):\n",
        "    def __init__(self):\n",
        "        self.x_train = torch.randn((3, 4))\n",
        "        self.y_train = torch. randn((3, 4))\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        x = torch.FloatTensor(self.x_train)\n",
        "        y = torch.FloatTensor(self.y_train)\n",
        "        return x, y\n",
        "\n",
        "    def __len__(self):\n",
        "        #dataset의 총 데이터 수\n",
        "        return len(self.x_data)\n",
        "\n",
        "def preprocess(batch):\n",
        "    pass\n",
        "\n",
        "########################################################################\n",
        "\n",
        "nb_epochs = 2000\n",
        "\n",
        "dataset = practiceDataset()\n",
        "data_loader = DataLoader(dataset=dataset\n",
        "                         batch_size=batch_num\n",
        "                         shuffle=True\n",
        "                         collate_fn=preprocess)\n",
        "\n",
        "for epoch in range(nb_epochs + 1):\n",
        "    for idx, (input_x, input_y) in enumerate(data_loader):"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}